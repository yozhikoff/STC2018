{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import random\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name, mfcc_count=40):\n",
    "        \n",
    "#     Нагенерим кучу всяких фич, в своем большенстве основанных на спектрограммах\n",
    "#     Вообще именно выделение фич кажется мне наиболее важным аспектом этой задачи,\n",
    "#     хотя может я просто использую простоватую модель\n",
    "    \n",
    "    \n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=mfcc_count).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    chroma_cens = librosa.feature.chroma_cens(y=X, sr=sample_rate, n_chroma=20).mean(axis=1)\n",
    "    tempo = librosa.feature.rhythm.tempogram(y=X, sr=sample_rate, win_length=50).mean(axis=1)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
    "    \n",
    "    return np.concatenate((mfccs, chroma, mel, contrast, chroma_cens, tempo, tonnetz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_wav(frames, name, sample_rate=16000):\n",
    "#     Научимся записывать .wav\n",
    "    f = wave.open(name, 'wb')\n",
    "    f.setnchannels(1)\n",
    "    f.setsampwidth(2)\n",
    "    f.setframerate(sample_rate)\n",
    "    f.setnframes(int(len(frames)))\n",
    "    f.writeframes(frames)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_wav(frames):\n",
    "    \n",
    "#     Это именно тот детектор расположения события, который должен повышать точность.\n",
    "#     И он действительно ее повышает! Просто обрежем ту часть, где тихо.\n",
    "    \n",
    "    maximum = max(frames)\n",
    "    \n",
    "    for i in range(len(frames)):\n",
    "        if frames[i] > maximum/13:\n",
    "            start = i\n",
    "            break\n",
    "    \n",
    "    for i in range(len(frames)):\n",
    "        if frames[::-1][i] > maximum/13:\n",
    "            stop = i\n",
    "            break\n",
    "            \n",
    "    return start, len(frames) - stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_classes = {}\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "#Считаем файл мета-данных и создадим словарь с именами файлов в качестве ключей и метками\n",
    "# в качестве значений\n",
    "\n",
    "with open(\"data_v_7_stc/meta/meta.txt\") as f:\n",
    "    for line in f:\n",
    "        file_name = line.split()[0]\n",
    "        file_class = line.split()[-1]\n",
    "        files_classes[file_name] = file_class\n",
    "        \n",
    "unique_classes = ['tool', 'keyboard', 'door', 'knocking_door', 'bags', 'speech', 'background', 'ring']\n",
    "files_class_numbers = {i: unique_classes.index(files_classes[i]) for i in files_classes.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обрежем все файлы и сохраним их в отдельную папку, файлы с речью трогать не будем,\n",
    "# им уготована иная судьба\n",
    "\n",
    "def cut_files(input_path, output_path, check_speech=False):\n",
    "\n",
    "    for i in tqdm(os.listdir(path=input_path)):\n",
    "        \n",
    "        if check_speech and files_classes[i] == \"speech\":\n",
    "            continue\n",
    "\n",
    "        file = wave.open(input_path + i)\n",
    "        sample_rate = file.getframerate()\n",
    "        frames = file.readframes(file.getnframes())\n",
    "        frames_int16 = (np.frombuffer(frames, dtype=np.int16))\n",
    "        start, stop = cut_wav(frames_int16)\n",
    "        write_wav(frames[start * 2 : stop * 2], output_path + i, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"new_data\")\n",
    "os.makedirs(\"new_test_data\")\n",
    "cut_files(\"data_v_7_stc/audio/\", \"new_data/\", check_speech=True)\n",
    "cut_files(\"data_v_7_stc/test/\", \"new_test_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Всего у нас около 250 файлов с речью в обучающей выборке, и это при том, что файлов с\n",
    "# дверьми более 3000! Удручающий дисбаланс. Воспользуемся тем, что файлы с речью длинные - \n",
    "# просто разрежем их на кусочки по 6 секунд и получим классный датасет.\n",
    "# На таком и обучаться сплошное удовольствие!\n",
    "\n",
    "speech_files = [i for i in files_classes.keys() if files_classes[i] == \"speech\"]\n",
    "speech_data = b\"\"\n",
    "\n",
    "for i in tqdm(speech_files):\n",
    "    if i in os.listdir(\"data_v_7_stc/audio/\"):\n",
    "        f = wave.open(\"data_v_7_stc/audio/\"+i)\n",
    "        speech_data += f.readframes(f.getnframes())\n",
    "        \n",
    "    \n",
    "    \n",
    "fps = 16000*2\n",
    "speech_files_count = len(speech_data)/fps//6\n",
    "speech_files_count = int(speech_files_count)\n",
    "    \n",
    "os.makedirs(\"new_speech\")\n",
    "\n",
    "for i in tqdm(range(speech_files_count)):\n",
    "    write_wav(speech_data[i*fps*6:(i+1)*fps*6], \"new_speech/test_speech{}.wav\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ну, собственно, генерим фичи, тысячи их! Самое долгое во всем ноутбуке.\n",
    "# Минут 30 тут придется подождать - а пока можно прочитать годный пост на хабре - \n",
    "# https://habr.com/post/351924/ \n",
    "# Вообще все посты от этого товарища - годнота высшей категории, очень советую\n",
    "\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "X_file_names = []\n",
    "for i in tqdm(os.listdir(path=\"new_data/\")):\n",
    "    try:\n",
    "        X_file_names.append(i)\n",
    "        X.append(extract_feature(\"new_data/\"+i, mfcc_count=60))\n",
    "        y.append(files_class_numbers[i])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "X = [X[i] for i in range(len(X)) if y[i] is not 5]\n",
    "X_file_names = [X_file_names[i] for i in range(len(X)) if y[i] is not 5]\n",
    "y = [i for i in y if i is not 5]\n",
    "\n",
    "for i in tqdm(os.listdir(path=\"new_speech/\")):\n",
    "    try:    \n",
    "        X_file_names.append(i)\n",
    "        X.append(extract_feature(\"new_speech/\"+i, mfcc_count=60))\n",
    "        y.append(unique_classes.index(\"speech\"))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "X_final = []\n",
    "X_final_names = []\n",
    "\n",
    "for i in tqdm(os.listdir(path=\"new_test_data/\")):\n",
    "    try:    \n",
    "        X_final.append(extract_feature(\"new_test_data/\"+i, mfcc_count=60))\n",
    "        X_final_names.append(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.0015, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(X_train), shuffle=True)\n",
    "\n",
    "test_dataset = MyDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(X_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Могу поставить на то, что в каждое второе решение будет основано на RNN и будет\n",
    "#заставлять краснеть меня с моей полносвязной сеточкой. Но результат, в общем-то,\n",
    "#совсем не плохой, так что мне даже не пришлось лезь в реккурентные дебри, \n",
    "#писать которые, к слову, не очень приятно на pytorch'e\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, out_count=8, in_count=len(X_scaled[0])):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "                nn.Linear(in_count, 500),\n",
    "                nn.BatchNorm1d(500),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.45),\n",
    "                nn.Linear(500, 1500),\n",
    "                nn.BatchNorm1d(1500),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.45),\n",
    "                nn.Linear(1500, 200),\n",
    "                nn.BatchNorm1d(200),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.45),\n",
    "                nn.Linear(200, out_count),\n",
    "                nn.Softmax())\n",
    "        \n",
    "        self.loss_test = []\n",
    "        self.loss_train = []\n",
    "\n",
    "        self.acc_test = []\n",
    "        self.acc_train = []\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(net, epoch_count, train_loader, test_loader, learning_rate=0.00001, plot = True):\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    for i in range(epoch_count):\n",
    "\n",
    "\n",
    "        #test\n",
    "        running_loss = []\n",
    "        running_acc = []\n",
    "        \n",
    "        for sample, label in test_loader:\n",
    "            data = sample.to(device)\n",
    "            label = label.to(device)\n",
    "            net.eval()\n",
    "            out = net(data)\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            loss_t = loss_fn(out, label)\n",
    "            running_loss.append(loss_t.item())\n",
    "            running_acc.append(sum((predicted == label).tolist())/len(predicted))\n",
    "\n",
    "        if plot: print(\"---------- \\n test\")\n",
    "        acc = np.mean(running_acc)\n",
    "        loss_ = np.mean(running_loss)\n",
    "        net.acc_test.append(acc)\n",
    "        if plot: print(acc)\n",
    "        net.loss_test.append(loss_)\n",
    "        \n",
    "        \n",
    "        #train\n",
    "        running_loss = []\n",
    "        running_acc = []\n",
    "        \n",
    "        \n",
    "        for sample, label in train_loader:\n",
    "            #augmentation_mask = torch.normal(torch.ones(sample.shape), torch.ones(sample.shape)*0.05)\n",
    "            #sample = sample*augmentation_mask\n",
    "            data = (sample).to(device)\n",
    "            label = label.to(device)\n",
    "            net.train()\n",
    "            net.zero_grad()\n",
    "            out = net(data)\n",
    "            loss = loss_fn(out, label)\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            loss.backward()\n",
    "            opt.step()  \n",
    "            running_loss.append(loss.item())\n",
    "            running_acc.append(sum((predicted == label).tolist())/len(predicted))\n",
    "        if plot: print(\"---------- \\n train\")\n",
    "        acc = np.mean(running_acc)\n",
    "        loss_ = np.mean(running_loss)\n",
    "        net.acc_train.append(acc)\n",
    "        if plot: print(acc)\n",
    "        net.loss_train.append(loss_)\n",
    "\n",
    "        if plot and i%10 == 0:\n",
    "            plt.figure(figsize=(8,7))\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.title(\"Loss\")\n",
    "            plt.plot(net.loss_test, color=\"orange\", label=\"test\")\n",
    "            plt.plot(net.loss_train, color=\"blue\", label=\"train\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.subplot(2,1,2)\n",
    "            plt.title(\"Accuracy\")\n",
    "            plt.plot(net.acc_test, color=\"orange\", label=\"test\")\n",
    "            plt.plot(net.acc_train, color=\"blue\", label=\"train\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(net, 2500, train_loader, test_loader, learning_rate=0.00003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_net = net\n",
    "tmp_net.eval()\n",
    "X_final_scale = scaler.transform(X_final)\n",
    "out = tmp_net((torch.FloatTensor(X_final_scale)).to(device))\n",
    "_, predicted = torch.max(out, 1)\n",
    "class_prob = list(map(max, out.tolist()))\n",
    "ans = []\n",
    "for i in range(len(predicted)):\n",
    "    ans.append((X_final_names[i], str(class_prob[i])[:6], unique_classes[predicted[i].item()]))\n",
    "    \n",
    "for i in sorted(ans, key=lambda x: x[1]):\n",
    "    print (*i, sep=\"\\t\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Решение открытой задачи\n",
    "\n",
    "#Самым нормальным решением оказалось сделать следующее - обучим восемь сеток, каждая\n",
    "# будет говорить, относится ли звук к конкретному классу или нет\n",
    "# Потом пошаманим немного с выводом и получим большую часть unknown'ов\n",
    "# Качество такого решения сильно превышает наивное \"возьмем те файлы, на которых нейронка \n",
    "# дает низкую интенсивность вывода\". Хотя и мое решение далеко от идеала."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_array = []\n",
    "loaders_array = []\n",
    "labels_array_binary = []\n",
    "for i in range(8):\n",
    "    y_binary = [1 if j==i else 0 for j in y]\n",
    "    labels_array_binary.append(y_binary)\n",
    "    X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X_scaled, y_binary, test_size=0.001, shuffle = True)\n",
    "    train_dataset_binary = MyDataset(X_train_binary, y_train_binary)\n",
    "    train_loader_binary = DataLoader(train_dataset_binary, batch_size=len(X_train_binary), shuffle=True)\n",
    "    test_dataset_binary = MyDataset(X_test_binary, y_test_binary)\n",
    "    test_loader_binary = DataLoader(test_dataset_binary, batch_size=len(X_test_binary), shuffle=True)\n",
    "    loaders_array.append((train_loader_binary, test_loader_binary))\n",
    "    net_array.append(Net(out_count=2, in_count=len(train_dataset_binary.X[0])).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    train_model(net_array[i], 70, loaders_array[i][0], loaders_array[i][1], learning_rate=0.0003)\n",
    "    #train_model(net_array[i], 10, loaders_array[i][0], loaders_array[i][1], learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_arr = []\n",
    "for i in range(8):\n",
    "    tmp_net = net_array[i]\n",
    "    tmp_net.eval()\n",
    "    X_final_scale = scaler.transform(X_final)\n",
    "    out = tmp_net((torch.FloatTensor(X_final_scale)).to(device))\n",
    "    _, predicted = torch.max(out, 1)\n",
    "    sum_arr.append(out)\n",
    "s = (sum_arr[0]+sum_arr[1]+sum_arr[2]+sum_arr[3]+sum_arr[4]+sum_arr[5]+sum_arr[6]+sum_arr[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_bin=[]\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "for i, j in zip(X_final_names, s):\n",
    "    ans_bin.append((i,j.tolist()[0]))\n",
    "\n",
    "for i in sorted(ans_bin, key=itemgetter(1, 0)):\n",
    "    print (*i, sep=\"\\t\")\n",
    "\n",
    "ans_bin = sorted(ans_bin, key=itemgetter(1, 0))\n",
    "unknown_array = [i[0] for i in ans_bin[:7]] + [i[0] for i in ans_bin[::-1][:60]]\n",
    "known_array = set(X_final_names) - set(unknown_array)\n",
    "\n",
    "ans_bin = ([i + \"\\t\" + \"1\" + \"\\n\" for i in sorted(unknown_array)[::-1]] +\n",
    "                                [i + \"\\t\" + \"0\" + \"\\n\" for i in known_array])\n",
    "\n",
    "with open(\"result_open_task.txt\", \"w\") as f:\n",
    "    f.writelines(ans_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ans = []\n",
    "for i in ans:\n",
    "    if i[0] not in unknown_array and float(i[1]) > 0.45:\n",
    "        new_ans.append(i)\n",
    "    else:\n",
    "        new_ans.append((i[0], \"0.0001\", i[2]))\n",
    "\n",
    "\n",
    "\n",
    "new_ans = [\"\\t\".join(i)+\"\\n\" for i in sorted(new_ans, key=itemgetter(0, 1))]\n",
    "\n",
    "with open(\"result.txt\", \"w\") as f:\n",
    "    f.writelines(new_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ну вот и все. Спасибо за чтение и за проведение такого интересного конкурса!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
